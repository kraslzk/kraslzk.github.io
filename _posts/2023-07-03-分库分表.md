<a name="kfYQA"></a>
## 数据库分库分表相关问题
<a name="akr8x"></a>
### 分库分表之后业务如何平滑上线？
<a name="jVfVJ"></a>
#### 步骤一，上线双写
，双写，即同时操作原表和分库分表之后的表。比如user_id=123，对于增加，删除，修改操作来说，咱们既操作user表，也操作user_id=123对应的user3表。因为查询的部分还是在user表中查询的，所以上面的操作对线上用户是无任何影响的。
<a name="hwTiq"></a>
#### 步骤二，全量同步
写一个全量同步user表到user1-user10的表，最好找个低峰期执行脚本，以防万一影响user表的查询<br />这一步执行之后，因为之前上线了双写，所以user表和user1-user10表之间的数据已经是完全一致的了。
<a name="Y1jDB"></a>
#### 步骤三，查询新表数据
将查询的部分改到user1-user10。因为前面两个步骤咱们已经保证了user表和各个分表之间的数据完全一致性，所以直接把查询的部分改掉是没有任何问题的。
<a name="HjyCT"></a>
### 常用分布式事务解决方案
<a name="irlFf"></a>
#### 2PC（两阶段提交）
两阶段提交，是一种在多节点间实现事务原子提交的算法，用来确保所有节点要么全部提交，要么全部中止。<br />**准备阶段：** 协调者会在准备阶段给所有参与者都发送准备命令。如果参与者发现准备命令无法执行或者执行失败时，可以返回失败，如果执行完成则保存事务日志并返回成功。针对于数据库的操作，准备阶段会记录redolog以及undolog为后续的提交做准备。需要注意的是**在准备阶段时，数据实际是没有被真正保存的**。<br />**提交阶段：** 协调者在提交阶段会根据准备阶段各个参与者的返回结果，判断是执行事务还是回滚事务并向所有参与者发起提交命令。其中只有当**准备阶段的所有节点都返回成功**，协调者才会发送执行事务的命令。如果有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。
<a name="DEERk"></a>
#### TCC
<a name="WFSU7"></a>
### 数据切分
关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当**单表的数据量达到1000W或100G以后**，由于查询维 度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。此时就要考虑对其进行切分了，切分的目的就在于减少数据库的负担，缩短查询时间。<br />**数据库分布式核心内容无非就是数据切分（Sharding），以及切分后对数据的定位、整合。**数据切分就是将数据分散存储到多个数据库中，使得单一数据库中的数据量变小，通过扩充主机的数量缓解单一数据库的性能问题，从而达到提升数据库操作性能的目的。数据切分根据其切分类型，可以分为两种方式：垂直（纵向）切分和水平（横向）切分。
<a name="zRP27"></a>
### 垂直（纵向）切分
垂直切分常见有垂直分库和垂直分表两种。<br />**垂直分库**就是根据业务耦合性，**将关联度低的不同表存储在不同的数据库**。做法与大系统拆分为多个小系统类似，按业务分类进行独立划分。与"微服务治理"的做法相似，每个微服务使用单独的一个数据库。<br />**垂直分表**是**基于数据库中的"列"进行**，某个表字段较多，可以**新建一张扩展表，将不经常用或字段长度较大的字段拆分出去到扩展表中**。在字段很多的情况下（例 如一个大表有100多个字段），通过"大表拆小表"，更便于开发与维护，也能避免跨页问题，MySQL底层是通过数据页存储的，一条记录占用空间过大会导 致跨页，造成额外的性能开销。另外数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了 磁盘IO，从而提升了数据库性能。

- 部分表无法join，只能通过接口聚合方式解决，提升了开发的复杂度
- 分布式事务处理复杂
- 依然存在单表数据量过大的问题（需要水平切分）
<a name="BtvF0"></a>
### 水平（横向）切分
水平切分分为库内分表和分库分表，是根据表内数据内在的逻辑关系，将同一个表按不同的条件分散到多个数据库或多个表中，**每个表中只包含一部分数据，从而使得单个表的数据量变小**，达到分布式的效果。

- 跨分片的事务一致性难以保证
- 跨库的join关联查询性能较差
- 数据多次扩展难度和维护量极大
<a name="Crjco"></a>
### 1、用户中心业务场景
用户中心是一个非常常见的业务，主要提供用户注册、登录、查询/修改等功能，其核心表为：<br />![](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702216356863-06876906-47ea-41b7-a050-a380fc3e8f60.png#averageHue=%23f5f4f3&clientId=uda76d634-0bd2-4&from=paste&id=u280aabe7&originHeight=108&originWidth=724&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=uef1ae4ed-5dfb-489f-a0f9-1c0dc6681f8&title=)<br />任何脱离业务的架构设计都是耍流氓，在进行分库分表前，需要对业务场景需求进行梳理：

- 用户侧：前台访问，访问量较大，需要保证高可用和高一致性。主要有两类需求：
- 用户登录：通过login_name/phone/email查询用户信息，1%请求属于这种类型
- 用户信息查询：登录之后，通过uid来查询用户信息，99%请求属这种类型
- 运营侧：后台访问，支持运营需求，按照年龄、性别、登陆时间、注册时间等进行分页的查询。是内部系统，访问量较低，对可用性、一致性的要求不高。
<a name="C75Dl"></a>
### 2、水平切分方法
**切分方法有"根据数值范围"和"根据数值取模"。**<br />"**根据数值范围**"：以主键uid为划分依据，按uid的范围将数据水平切分到多个数据库上。例如：user-db1存储uid范围为0~1000w的数据，user-db2存储uid范围为1000w~2000wuid数据。

- 优点是：扩容简单，如果容量不够，只要增加新db即可。
- 不足是：**请求量不均匀**，一般新注册的用户活跃度会比较高，所以新的user-db2会比user-db1负载高，导致服务器利用率不平衡

"**根据数值取模**"：也是以主键uid为划分依据，按uid取模的值将数据水平切分到多个数据库上。例如：user-db1存储uid取模得1的数据，user-db2存储uid取模得0的uid数据。

- 优点是：数据量和请求量分布均均匀
- 不足是：**扩容麻烦**，当容量不够时，新增加db，需要rehash。需要考虑对数据进行平滑的迁移。

**一致性HASH**<br />按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，即0~ (2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形，<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702217455934-f2fe3a66-631a-47e2-aff4-30118d35ab70.png#averageHue=%23f8f8f8&clientId=uda76d634-0bd2-4&from=paste&height=298&id=u7d30c312&originHeight=451&originWidth=806&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=177559&status=done&style=none&taskId=u52be2482-e1eb-463c-a1fa-ce11db5300e&title=&width=532.7999877929688)<br />这个圆环首尾相连，那么假设现在有三个数据库服务器节点node1、node2、node3三个节点，每个节点负责自己这部分的用户数据存储，假设有用户user1、user2、user3，我们可以对服务器节点进行HASH运算，假设HASH计算后，user1落在node1上，user2落在node2上，user3落在user3上。假设node3节点失效了，user3将会落到node1上，而之前的node1和node2数据不会改变，再假设新增了节点node4，发现user3会落到node4上。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。
<a name="WomwD"></a>
### 3、非uid的查询方法
水平切分后，对于按uid查询的需求能很好的满足，可以直接路由到具体数据库。而按非uid的查询，例如**login_name**，就**不知道具体该访问哪个库了，此时需要遍历所有库**，性能会降低很多。<br />对于用户侧，可以采用"**建立非uid属性到uid的映射关系**"的方案；对于运营侧，可以采用"**前台与后台分离**"的方案。
<a name="PdFkk"></a>
#### 3.1、建立非uid属性到uid的映射关系
例如：login_name不能直接定位到数据库，可以**建立login_name→uid的映射关系**，用**索引表或缓存**来存储。当**访问login_name时，先通过映射表查询出login_name对应的uid，再通过uid定位到具体的库**。<br />映射表只有两列，可以承载很多数据，当数据量过大时，也可以对映射表再做水平切分。这类kv格式的索引结构，可以很好的使用cache来优化查询性能，而且映射关系不会频繁变更，**缓存命中率会很高**。
<a name="aBnWP"></a>
##### 3.1.1建立映射索引表
思路：userId直接定位到库，userName不能直接定位到库，如果通过userName能查询到userId，问题解决。<br />解决方案：<br />1）建立一个索引表记录userName->userId的映射关系<br />2）用userName来访问时，先通过索引表查询到userId，再定位相应的库<br />3）索引表属性较少，可以容纳非常多数据，一般不需要分库<br />4）如果数据量过大，可以通过userName来分库<br />潜在不足：多一次数据库查询，性能下降一倍。
<a name="tf50P"></a>
##### 3.1.2将映射缓存到cache如（Redis）
思路：访问索引表性能较低，把映射关系放在缓存里性能更佳。<br />解决方案：<br />1）userName查询先到cache中查询userId，再根据userId定位数据库<br />2）假设cache miss，采用扫全库法获取userName对应的userId，放入cache<br />3）userName到userId的映射关系不会变化，映射关系一旦放入缓存，不会更改，无需淘汰，缓存命中率超高<br />4）如果数据量过大，可以通过userName进行cache水平切分<br />潜在不足：多一次cache查询
<a name="XEEeQ"></a>
#### 3.2、前台与后台分离
对于用户侧，主要需求是以**单行查询**为主，需要**建立login_name/phone/email到uid的映射关系**，可以解决这些字段的查询问题。<br />而对于运营侧，很多批量分页且条件多样的查询，这类查询计算量大，返回数据量大，对数据库的性能消耗较高。此时，如果和用户侧公用同一批服务或数据库，可能因为后台的少量请求，占用大量数据库资源，而导致用户侧访问性能降低或超时。<br />这类业务最好采用"前台与后台分离"的方案，运营侧后台业务抽取独立的service和db，解决和前台业务系统的耦合。由于运营侧对可用 性、一致性的要求不高，可以不访问实时库，而是通过binlog异步同步数据到运营库进行访问。在数据量很大的情况下，还可以使用ES搜索引擎或Hive 来满足后台复杂的查询方式。
<a name="Htoba"></a>
#### 3.3基因法将非id字段hash融入id字段
思路：不能用userName生成userId，可以从userName抽取“基因”，融入userId中<br />假设分8库，采用userId%8路由，潜台词是，userId的最后3个bit决定这条数据落在哪个库上，这3个bit就是所谓的“基因”。<br />解决方案：<br />1）在用户注册时，设计函数userName生成3bit基因，userName_gene=f(userName)<br />2）同时，生成61bit的全局唯一id，作为用户的标识<br />3）接着把3bit的userName_gene也作为userId的一部分<br />4）生成64bit的userId，由id和userName_gene拼装而成，并按照userId库插入数据<br />5）用userName来访问时，先通过函数由userName再次复原3bit基因，userName_gene=f(userName)，通过userName_gene%8直接定位到库
<a name="KZK5m"></a>
### 分库分表策略

- **数据量大，选分表；**
- **并发高，选分库；**
- **海量存储+高并发，分库+分表。**
<a name="kHVsf"></a>
#### 分片键的选择

1. 选择具有**共性的字段**作为分片键，即查询中**高频出现的条件字段**；
2. 分片字段应具有**高度离散**的特点，**分片键的内容不能被更新**；
3. 可均匀各分片的数据存储和读写压力，避免片内出现热点数据；
4. 尽量减少单次查询所涉及的分片数量，降低数据库压力；
5. 最后，**不要更换分片键**，更换分片键需重分布数据，代价较大。
<a name="WaToZ"></a>
#### 基因法+冗余数据
在超大规模的电商场景中，使用**订单 ID 和买家 ID 查询数据**的问题。大规模电商场景每日会产生海量的交易订单，在这个场景中，我们选择使用**订单 ID 作为分片键**是一个没有异议的选择。那么此时，我们通过 APP 来查询自己的订单时，**查询条件变为了分片键之外的买家 ID**，默认情况下，查询语句中不带有分片键会导致**全路由**情况。面对这样的情况，大厂常常使用的方案是基因法，即**将买家 ID 融入到订单 ID 中，作为订单 ID 后缀**。这样，指定买家的所有订单就会与其订单在同一分片内了，<br />![](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702219010413-21d712d0-161d-4f44-bf81-12b01f08d781.png#averageHue=%23b0b0b0&clientId=uda76d634-0bd2-4&from=paste&height=332&id=u30ace7da&originHeight=896&originWidth=1298&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=u55df2e12-c866-4347-86a0-9edbb262b1d&title=&width=481.4000244140625)
<a name="yKkpz"></a>
### SharingSphere
参考[https://juejin.cn/post/6890772387000762382?from=search-suggest](https://juejin.cn/post/6890772387000762382?from=search-suggest)
<a name="Y0eZp"></a>
#### 分片
一般我们在提到分库分表的时候，大多是以水平切分模式（水平分库、分表）为基础来说的，数据分片将原本一张数据量较大的表 t_order 拆分生成数个表结构完全一致的小数据量表 t_order_0、t_order_1、···、t_order_n，每张表只存储原大表中的一部分数据，当执行一条SQL时会通过 分库策略、分片策略 将数据分散到不同的数据库、表内。
<a name="FGkyi"></a>
#### 数据节点
数据节点是分库分表中一个不可再分的最小数据单元（表），它由**数据源名称**和**数据表**组成，例如 order_db_1.t_order_0、order_db_2.t_order_1 就表示一个数据节点。
<a name="T9qDF"></a>
#### 逻辑表
逻辑表是指一组具有相同逻辑和数据结构表的总称。比如我们将订单表t_order 拆分成 t_order_0 ···  t_order_9 等 10张表。此时我们会发现分库分表以后数据库中已不在有 t_order 这张表，取而代之的是 t_order_n，但我们在**代码中写 SQL 依然按 t_order 来写**。此时 t_order 就是这些拆分表的逻辑表。
<a name="S5aAz"></a>
#### 真实表
真实表也就是上边提到的 t_order_n 数据库中真实存在的物理表。
<a name="NJzRI"></a>
#### 分片键
用于分片的数据库字段。我们将 t_order 表分片以后，当执行一条SQL时，通过对字段 order_id 取模的方式来决定，这条数据该在哪个数据库中的哪个表中执行，此时 order_id 字段就是 t_order 表的分片健。
<a name="wKCSa"></a>
#### sharding-jdbc 提供了4种分片算法
<a name="AJfDb"></a>
##### 1、精确分片算法
精确分片算法（PreciseShardingAlgorithm）用于**单个字段**作为分片键，SQL中有 = 与 IN 等条件的分片，需要在标准分片策略（StandardShardingStrategy ）下使用。<br />实现自定义精准分库、分表算法的方式大致相同，都要实现 PreciseShardingAlgorithm 接口，并重写 doSharding() 方法。

```java
public class MyDBPreciseShardingAlgorithm implements PreciseShardingAlgorithm<Long> {
     @Override
    public String doSharding(Collection<String> databaseNames, PreciseShardingValue<Long> shardingValue) {
        /**
         * databaseNames 所有分片库的集合
         * shardingValue 为分片属性，其中 logicTableName 为逻辑表，columnName 分片健（字段），
         value 为从 SQL 中解析出的分片健的值
         */
    }
}
```

```java

public class MyTablePreciseShardingAlgorithm implements PreciseShardingAlgorithm<Long> {

    @Override
    public String doSharding(Collection<String> tableNames, PreciseShardingValue<Long> shardingValue) {

        /**
         * tableNames 对应分片库中所有分片表的集合
         * shardingValue 为分片属性，其中 logicTableName 为逻辑表，
         columnName 分片健（字段），value 为从 SQL 中解析出的分片健的值
         */
    }
}
```
<a name="EwaSk"></a>
##### 2、范围分片算法
范围分片算法（RangeShardingAlgorithm）用于**单个字段**作为分片键，SQL中有 BETWEEN AND、>、<、>=、<=  等条件的分片，需要在标准分片策略（StandardShardingStrategy ）下使用。
<a name="gZwcs"></a>
##### 3、复合分片算法
复合分片算法（ComplexKeysShardingAlgorithm）用于**多个字段作为分片键**的分片操作，同时获取到多个分片健的值，根据多个字段处理业务逻辑。需要在复合分片策略（ComplexShardingStrategy ）下使用。
<a name="ElpdA"></a>
##### 4、Hint分片算法
Hint分片算法（HintShardingAlgorithm）稍有不同，上边的算法中我们都是解析SQL 语句提取分片键，并设置分片策略进行分片。但有些时候我们并没有使用任何的分片键和分片策略，可还想将 SQL 路由到目标数据库和表，就需要通过手动干预指定SQL的目标数据库和表信息，这也叫强制路由。
<a name="cXq6j"></a>
#### 分片策略
分片策略是一种抽象的概念，实际分片操作的是由分片算法和分片健来完成的。
<a name="YkVz6"></a>
##### 1、标准分片策略
标准分片策略适用于单分片键，此策略支持 PreciseShardingAlgorithm 和 RangeShardingAlgorithm 两个分片算法。<br />其中 PreciseShardingAlgorithm 是必选的，用于处理 = 和 IN 的分片。RangeShardingAlgorithm 是可选的，用于处理BETWEEN AND， >， <，>=，<= 条件分片，如果不配置RangeShardingAlgorithm，SQL中的条件等将按照全库路由处理。
<a name="PBMXJ"></a>
##### 2、复合分片策略
复合分片策略，同样支持对 SQL语句中的 =，>， <， >=， <=，IN和 BETWEEN AND 的分片操作。不同的是它支持多分片键，具体分配片细节完全由应用开发者实现。
<a name="ZW5C1"></a>
##### 3、行表达式分片策略
行表达式分片策略，支持对 SQL语句中的 = 和 IN 的分片操作，但只支持单分片键。这种策略通常用于简单的分片，不需要自定义分片算法，可以直接在配置文件中接着写规则。t_order_$->{t_order_id % 4} 代表 t_order 对其字段 t_order_id取模，拆分成4张表，而表名分别是t_order_0 到 t_order_3。
<a name="a5zGh"></a>
#### 如果我一部分表做了分库分表，另一部分未做分库分表的表怎么处理？

- 严格划分功能库，分片的库与不分片的库剥离开，业务代码中按需切换数据源访问
- 设置默认数据源，以 Sharding-JDBC 为例，不给未分片表设置分片规则，它们就不会执行，因为找不到路由规则，这时我们设置一个默认数据源，在找不到规则时一律访问默认库。


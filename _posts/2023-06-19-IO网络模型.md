<a name="29250454"></a>
### 2.1 用户空间和内核态空间
进程的寻址空间划分成两部分：**内核空间、用户空间**

什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4GB，这个4GB，会有3个GB分给用户空间，会有1GB给内核系统

在linux中，他们权限分成两个等级，0和3，用户空间只能执行受限的命令（Rank3），而且不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令（Rank0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。

比如：<br />Linux系统为了提高IO效率，会在用户空间和内核空间都加入缓冲区：<br />写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备<br />读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区

针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。<br />在《UNIX网络编程》一书中，总结归纳了5种IO模型：

- 阻塞IO（Blocking IO）
- 非阻塞IO（Nonblocking IO）
- IO多路复用（IO Multiplexing）
- 信号驱动IO（Signal Driven IO）
- 异步IO（Asynchronous IO）
<a name="fd79c901"></a>
### 2.2.网络模型-阻塞IO(BIO)
应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。<br />具体流程如下图：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702370305733-37f6b3a1-65f0-49a3-b2cf-2cfa8b301b66.png#averageHue=%23e5eed6&clientId=udab92dd4-9bf0-4&from=paste&height=254&id=u88538f37&originHeight=317&originWidth=673&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=88890&status=done&style=none&taskId=u9d1a2ae0-ea2b-4117-92cf-ade9054883e&title=&width=538.4)<br />用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞IO。阻塞IO就是**两个阶段都必须阻塞等待**：<br />**阶段一：**

- 用户进程尝试读取数据（比如网卡数据）
- 此时数据尚未到达，内核需要等待数据
- 此时用户进程也处于阻塞状态

阶段二：

- 数据到达并拷贝到内核缓冲区，代表已就绪
- 将内核数据拷贝到用户缓冲区
- 拷贝过程中，用户进程依然阻塞等待
- 拷贝完成，用户进程解除阻塞，处理数据

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702370434117-766e64dd-8c1c-40d1-9937-bf45f6718c67.png#averageHue=%23fafafa&clientId=udab92dd4-9bf0-4&from=paste&height=410&id=ue7a5deed&originHeight=513&originWidth=1061&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=80823&status=done&style=none&taskId=ub818d79a-f869-4d47-9f92-279907ba2ec&title=&width=848.8)
<a name="3e7c12a2"></a>
### 2.3 网络模型-非阻塞IO
非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。<br />阶段一：

- 用户进程尝试读取数据（比如网卡数据）
- 此时数据尚未到达，内核需要等待数据
- **返回异常给用户进程**
- **用户进程拿到error后，再次尝试读取**
- **循环往复，直到数据就绪**

阶段二：

- 将内核数据拷贝到用户缓冲区
- 拷贝过程中，用户进程依然阻塞等待
- 拷贝完成，用户进程解除阻塞，处理数据
- 可以看到，非阻塞IO模型中，用户进程在**第一个阶段是非阻塞**，**第二个阶段是阻塞状态**。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致**CPU空转，CPU使用率暴增**。

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702370514397-c2977e88-0a6a-488c-a3f8-8b0f3896e5cb.png#averageHue=%23f6f6f6&clientId=udab92dd4-9bf0-4&from=paste&height=414&id=uead1fb8d&originHeight=518&originWidth=1031&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=80442&status=done&style=none&taskId=uf167bc78-a792-4cc9-99cf-aa50e5fa700&title=&width=824.8)
<a name="cf964268"></a>
### 2.4 网络模型-IO多路复用
参考[https://zhuanlan.zhihu.com/p/367591714](https://zhuanlan.zhihu.com/p/367591714)<br />无论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：<br />如果调用recvfrom时，恰好没有数据，阻塞IO会使CPU阻塞，非阻塞IO使CPU空转，都不能充分发挥CPU的作用。<br />如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据。<br />而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能自然会很差。

就比如服务员给顾客点餐，**分两步**：

- 顾客思考要吃什么（等待数据就绪）
- 顾客想好了，开始点餐（读取数据）

要提高效率有几种办法？<br />方案一：增加更多服务员（多线程）<br />方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（**用户应用就去读取数据**）

那么问题来了：用户进程如何知道内核中数据是否就绪呢？<br />这个问题的解决依赖于提出的**文件描述符（File Descriptor）**：简称FD，是一个从0 开始的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）。<br />通过FD，我们的网络模型可以**利用一个线程监听多个FD**，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。<br />阶段一：

- 用户进程调用select，指定要监听的FD集合
- 核监听FD对应的多个socket
- 任意一个或多个socket数据就绪则返回readable
- 此过程中用户进程阻塞

阶段二：

- 用户进程找到就绪的socket
- 依次调用recvfrom读取数据
- 内核将数据拷贝到用户空间
- 用户进程处理数据

当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果N多个FD一个都没处理完，此时就进行等待。

用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702370777144-3c82b982-8031-4648-b3e1-d04f535edae1.png#averageHue=%23f7f7f7&clientId=udab92dd4-9bf0-4&from=paste&height=397&id=ub016bdd9&originHeight=496&originWidth=1014&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=95824&status=done&style=none&taskId=u802c87bf-be2e-4dd3-919c-11139dcfc6f&title=&width=811.2)<br />IO多路复用是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。不过监听FD的方式、通知的方式又有多种实现，常见的有：

- select
- poll
- epoll

其中select和poll相当于是当被监听的数据准备好之后，他会把你监听的FD整个数据都发给你，你需要到整个FD中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好

而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。
<a name="f34f9474"></a>
### 2.5 网络模型-IO多路复用-select方式
select是Linux最早是由的I/O多路复用技术：<br />简单说，就是我们把**需要处理的数据封装成FD**，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个FD的最大值+1，但是大小整体是有限制的 ），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据，

比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将**整个fd发给内核态**，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再将处理掉没有准备好的数据，最后再**将这个FD集合写回到用户态中去**，此时用户态就知道了，奥，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以**用户态也需要去进行遍历**，然后找到对应准备好数据的节点，再去发起读请求，我们会发现，这种模式下他虽然比阻塞IO和非阻塞IO好，但是依然有些**麻烦的事情， 比如说频繁的传递fd集合，频繁的去遍历FD等问题**<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702371074790-72804cda-d1e7-46d0-bf51-75d0a2476e4d.png#averageHue=%23e7efe3&clientId=udab92dd4-9bf0-4&from=paste&height=510&id=u1401d7a8&originHeight=637&originWidth=1575&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=383966&status=done&style=none&taskId=u66d3b8b7-899b-4bab-b2b5-fbdb6ee6562&title=&width=1260)<br />![select.gif](https://cdn.nlark.com/yuque/0/2023/gif/40745172/1702372104121-db0105f6-f8fa-42f4-b3e5-5ac946e5c0c6.gif#averageHue=%23d3d4ca&clientId=udab92dd4-9bf0-4&from=paste&height=320&id=u35251abc&originHeight=400&originWidth=550&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=714304&status=done&style=none&taskId=u597e0892-62a6-4834-9602-fa9c83a5f26&title=&width=440)

<a name="ac8d619e"></a>
### 2.6 网络模型-IO多路复用模型-poll模式
poll模式对select模式做了简单改进，但性能提升不明显：<br />IO流程：

- 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义
- 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限
- 内核遍历fd，判断是否就绪
- 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n
- 用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd

**与select对比：**

- select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上**无上限**
- 监听FD越多，每次遍历消耗时间也越久，性能反而会下降

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702371233198-a4198203-07b2-4c39-a92c-d1ade702f463.png#averageHue=%23edf4eb&clientId=udab92dd4-9bf0-4&from=paste&height=433&id=ucce34dde&originHeight=541&originWidth=731&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=206620&status=done&style=none&taskId=u67d3511d-9714-4b26-b4d1-0307a14dd08&title=&width=584.8)<br />![](.%5C%E5%8E%9F%E7%90%86%E7%AF%87.assets%5C1653900721427.png#id=ihvHi&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
<a name="3e62f75c"></a>
### 2.7 网络模型-IO多路复用模型-epoll函数
epoll模式是对select和poll的改进，它提供了三个函数：<br />第一个是：eventpoll的函数，他内部包含两个东西<br />一个是：<br />1、红黑树-> 记录的事要监听的FD<br />2、链表->一个链表，记录的是就绪的FD<br />紧接着调用epoll_ctl操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听（回调）函数，这个函数会在fd数据就绪时触发，就是准备好了，现在就把fd把数据添加到list_head中去<br />3、调用epoll_wait函数<br />就去等待，在用户态创建一个空的events数组，当就绪之后，我们的回调函数会把数据添加到list_head中去，当调用这个函数的时候，会去检查list_head，当然这个过程需要参考配置的等待时间，可以等一定时间，也可以一直等， 如果在此过程中，检查到了list_head中有数据会将数据添加到链表中，此时将数据放入到events数组中，并且返回对应的操作的数量，用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据。

小总结：<br />select模式存在的三个问题：

- 能监听的FD最大不超过1024
- 每次select都需要把所有要监听的FD都拷贝到内核空间
- 每次都要遍历所有FD来判断就绪状态

poll模式的问题：

- poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有FD，如果监听较多，性能会下降

epoll模式中如何解决这些问题的？

- 基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增删改查效率都非常高
- 每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，**无需重复拷贝FD**到内核空间
- 利用ep_poll_callback机制来监听FD状态，**无需遍历所有FD**，因此性能不会随监听的FD数量增多而下降

![epoll.gif](https://cdn.nlark.com/yuque/0/2023/gif/40745172/1702372396834-ba1e8291-968d-47e0-9f71-f0a141f6139f.gif#averageHue=%23d3d4ca&clientId=udab92dd4-9bf0-4&from=paste&height=320&id=u38c95817&originHeight=400&originWidth=550&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=550346&status=done&style=none&taskId=udd5c62d5-4c3b-46d7-96b9-6d570f8d641&title=&width=440)
<a name="ad6c5f19"></a>
### 2.8、网络模型-epoll中的ET和LT

如何理解Epoll中的LT和ET模式，底层实现又是怎么样的？ [https://www.zhihu.com/question/403893498/answer/2258283710](https://www.zhihu.com/question/403893498/answer/2258283710)

当FD有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：

- LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知，即持续通知直到处理事件完毕。
- EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知，即只通知一次，不管事件是否处理完毕。

举个栗子：

- 假设一个客户端socket对应的FD已经注册到了epoll实例中
- 客户端socket发送了2kb的数据
- 服务端调用epoll_wait，得到通知说FD就绪
- 服务端从FD读取了1kb数据回到步骤3（再次调用epoll_wait，形成循环）

结论

如果我们采用LT模式，因为FD中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知<br />如果我们采用ET模式，因为第③步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。

<a name="666970a1"></a>
### 2.9 网络模型-基于epoll的服务器端流程

服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据<br />1、红黑树（为空）：rb_root 用来去记录需要被监听的FD<br />2、链表（为空）：list_head，用来存放已经就绪的FD

创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)

3、当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702379331609-9029e290-21be-4dbb-94de-7103b6af8419.png#averageHue=%23e0e9de&clientId=udab92dd4-9bf0-4&from=paste&height=511&id=u444b49ee&originHeight=639&originWidth=1528&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=207920&status=done&style=none&taskId=ud7063606-f3e7-4b31-8714-28ae535b2ec&title=&width=1222.4)<br />![](.%5C%E5%8E%9F%E7%90%86%E7%AF%87.assets%5C1653902845082.png#id=KFvRe&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
<a name="52bbf270"></a>
### 3.0 、网络模型-信号驱动

信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。<br />阶段一：

- 用户进程调用sigaction，注册信号处理函数
- 内核返回成功，开始监听FD
- 用户进程不阻塞等待，可以执行其它业务
- 当内核数据就绪后，回调用户进程的SIGIO处理函数

阶段二：

- 收到SIGIO回调信号
- 调用recvfrom，读取
- 内核将数据拷贝到用户空间
- 用户进程处理数据

![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702379463130-25d74b4a-d32f-4b4f-aebd-e854c21bd019.png#averageHue=%23f6f6f6&clientId=udab92dd4-9bf0-4&from=paste&height=421&id=uc2ed9200&originHeight=526&originWidth=937&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=122033&status=done&style=none&taskId=u59900219-e109-4518-b289-06925d1c0ee&title=&width=749.6)<br />当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。

<a name="d95b3e79"></a>
#### 3.0.1 异步IO

- 在IO模型里面如果请求方从发起请求到数据最后完成的这一段过程中都需要自己参与，那么这种我们称为同步；
- 如果应用发送完指令后就不再参与过程了，只需要等待最终完成结果的通知，那么这就属于异步。

这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞<br />他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702379669424-cd6aaae6-6a5e-4b61-b3cd-d64c1f234d93.png#averageHue=%23f9f9f9&clientId=udab92dd4-9bf0-4&from=paste&height=413&id=u8ad94e21&originHeight=516&originWidth=1094&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=80387&status=done&style=none&taskId=u6786c95c-a92e-48d9-9e96-336a8e9b740&title=&width=875.2)
<a name="5fa292df"></a>
#### 3.0.2 对比
![image.png](https://cdn.nlark.com/yuque/0/2023/png/40745172/1702379751325-905ac8ad-ea32-41c2-a37b-38fc0f2d2250.png#averageHue=%23ededed&clientId=udab92dd4-9bf0-4&from=paste&height=505&id=ub3057925&originHeight=631&originWidth=989&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=221728&status=done&style=none&taskId=u62613d48-4343-4b53-8906-e91cdbb3070&title=&width=791.2)
